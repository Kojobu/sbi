{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from sbi.types import Shape\n",
    "from pyknos.nflows.flows import Flow\n",
    "from sbi.neural_nets.density_estimators import DensityEstimator\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from sbi.utils import assert_all_finite, mog_log_prob\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDNDensityEstimator(DensityEstimator):\n",
    "    def __init__(self, flow: Flow, condition_shape: torch.Size) -> None:\n",
    "        super().__init__(flow, condition_shape)\n",
    "        self._logits = None\n",
    "        self._means = None\n",
    "        self._precisions = None\n",
    "\n",
    "    @property\n",
    "    def embedding_net(self) -> nn.Module:\n",
    "        r\"\"\"Return the embedding network.\"\"\"\n",
    "        return self.net._embedding_net\n",
    "    \n",
    "    @property\n",
    "    def distribution(self):\n",
    "        r\"\"\"Return the distribution of the density estimator.\"\"\"\n",
    "        return self.net._distribution\n",
    "    \n",
    "    @property\n",
    "    def mog_parameters(self):\n",
    "        r\"\"\"Return the parameters of the mixture of Gaussians.\"\"\"\n",
    "        if self._logits is None:\n",
    "            self.mog_parameters = self.distribution.get_mixture_components()\n",
    "        return self._logits, self._means, self._precisions\n",
    "    \n",
    "    @mog_parameters.setter\n",
    "    def mog_parameters(self, mog_parameters: Tuple[Tensor, Tensor, Tensor]):\n",
    "        r\"\"\"Set the parameters of the mixture of Gaussians.\"\"\"\n",
    "        self._logits, self._means, self._precisions = mog_parameters\n",
    "\n",
    "    @property\n",
    "    def logsumexplogits(self):\n",
    "        return torch.logsumexp(self._logits, dim=-1, keepdim=True)\n",
    "\n",
    "    def reset_mog_parameters(self):\n",
    "        r\"\"\"Reset the parameters of the mixture of Gaussians.\"\"\"\n",
    "        self.mog_parameters = self.distribution.get_mixture_components()\n",
    "\n",
    "    def mog_log_prob(self, input: Tensor) -> Tensor:\n",
    "        r\"\"\"Return the log probability of the input under the mixture of Gaussians.\"\"\"\n",
    "        return mog_log_prob(input, *self.mog_parameters)\n",
    "\n",
    "    def correct_for_proposal(self, proposal: MDNDensityEstimator, inplace: bool = True):\n",
    "        logits_d, m_d, prec_d = self.mog_parameters\n",
    "        logits_pp, m_pp, prec_pp = proposal.mog_parameters\n",
    "\n",
    "        logits_pp, m_pp, prec_pp = self._proposal_posterior_transformation(logits_pp, m_pp, prec_pp, logits_d, m_d, prec_d)\n",
    "        self.mog_parameters = (logits_pp, m_pp, prec_pp)\n",
    "\n",
    "    def condition(self, condition: Tensor, mask: List[bool] = None, inplace: bool = True) -> MDNDensityEstimator:\n",
    "        # return copy of the MDN with conditioned parameters\n",
    "        condition = atleast_2d_float32_tensor(condition)\n",
    "\n",
    "        dims_to_sample = torch.where(torch.tensor(mask))[0]\n",
    "        cond_logits, cond_means, cond_precfs, cond_sumlogdiag = condition_mog(condition, dims_to_sample, *self.mog_parameters)\n",
    "        cond_precs = cond_precfs.transpose(3, 2) @ cond_precfs\n",
    "        # TODO: CHECK IF THIS IS CORRECT OR cond_logits need to be normalized\n",
    "\n",
    "        if not inplace:\n",
    "            conditioned_mdn = deepcopy(self)\n",
    "            conditioned_mdn.mog_parameters = (cond_logits, cond_means, cond_precs)\n",
    "            return conditioned_mdn\n",
    "        self.mog_parameters = (cond_logits, cond_means, cond_precs)\n",
    "\n",
    "    def set_context(self, context: Tensor, inplace: bool = True):\n",
    "        self._check_condition_shape(context)\n",
    "        \n",
    "        embedded_x = self.embedding_net(context)\n",
    "        logits_d, m_d, prec_d, _, _ = self.distribution.get_mixture_components(embedded_x)\n",
    "        norm_logits_d = logits_d - torch.logsumexp(logits_d, dim=-1, keepdim=True)\n",
    "        if not inplace:\n",
    "            mdn = deepcopy(self)\n",
    "            mdn.mog_parameters = (norm_logits_d, m_d, prec_d)\n",
    "            return mdn\n",
    "        self.mog_parameters = (norm_logits_d, m_d, prec_d)\n",
    "\n",
    "    def marginalize(self, inplace: bool = True):\n",
    "        # return copy of the MDN with marginalized parameters\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, sample_shape: Shape, condition: Tensor) -> Tensor:\n",
    "        self._check_condition_shape(condition)\n",
    "        logits_p, m_p, prec_p = self.mog_parameters\n",
    "        prec_factors_p = torch.linalg.cholesky(prec_p, upper=True)\n",
    "\n",
    "        num_samples = torch.Size(sample_shape).numel()\n",
    "        batch_size = 1 if sample_shape.ndim == 1 else sample_shape[0]\n",
    "        # Replicate to use batched sampling from pyknos.\n",
    "        if batch_size is not None and batch_size > 1:\n",
    "            logits_p = logits_p.repeat(batch_size, 1)\n",
    "            m_p = m_p.repeat(batch_size, 1, 1)\n",
    "            prec_factors_p = prec_factors_p.repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "        # Get (optionally z-scored) MoG samples.\n",
    "        theta = self.distribution.sample_mog(num_samples, logits_p, m_p, prec_factors_p)\n",
    "        embedded_context = self.embedding_net(condition)\n",
    "        if embedded_context is not None:\n",
    "            # Merge the context dimension with sample dimension in order to\n",
    "            # apply the transform.\n",
    "            theta = torchutils.merge_leading_dims(theta, num_dims=2)\n",
    "            embedded_context = torchutils.repeat_rows(\n",
    "                embedded_context, num_reps=num_samples\n",
    "            )\n",
    "\n",
    "        theta, _ = self.net._transform.inverse(theta, context=embedded_context)\n",
    "\n",
    "        if embedded_context is not None:\n",
    "            # Split the context dimension from sample dimension.\n",
    "            theta = torchutils.split_leading_dim(theta, shape=[-1, num_samples])\n",
    "\n",
    "        return theta.reshape(*sample_shape,-1)\n",
    "\n",
    "    def log_prob(self, input: Tensor, condition: Tensor, proposal: Optional[MDNDensityEstimator]) -> Tensor:\n",
    "        self._check_condition_shape(condition)\n",
    "        condition_dims = len(self._condition_shape)\n",
    "\n",
    "        # PyTorch's automatic broadcasting\n",
    "        batch_shape_in = input.shape[:-1]\n",
    "        batch_shape_cond = condition.shape[:-condition_dims]\n",
    "        batch_shape = torch.broadcast_shapes(batch_shape_in, batch_shape_cond)\n",
    "        # Expand the input and condition to the same batch shape\n",
    "        input = input.expand(batch_shape + (input.shape[-1],))\n",
    "        condition = condition.expand(batch_shape + self._condition_shape)\n",
    "        # Flatten required by nflows, but now both have the same batch shape\n",
    "        input = input.reshape(-1, input.shape[-1])\n",
    "        condition = condition.reshape(-1, *self._condition_shape)\n",
    "\n",
    "        # z-score theta if it z-scoring had been requested.\n",
    "        theta = self._maybe_z_score_theta(input)\n",
    "\n",
    "        self.set_context(condition, inplace=True)\n",
    "        if proposal is not None:\n",
    "            self.correct_for_proposal(proposal)\n",
    "        log_probs = self.mog_log_prob(theta)\n",
    "        log_probs = log_probs.reshape(batch_shape) # \\hat{p} from eq (3) in [1]\n",
    "\n",
    "        assert_all_finite(log_probs, \"proposal posterior eval\")\n",
    "\n",
    "        return log_probs   \n",
    "\n",
    "    def loss(self, input: Tensor, condition: Tensor) -> Tensor:\n",
    "        r\"\"\"Return the loss for training the density estimator.\n",
    "\n",
    "        Args:\n",
    "            input: Inputs to evaluate the loss on of shape (batch_size, input_size).\n",
    "            condition: Conditions of shape (batch_size, *condition_shape).\n",
    "\n",
    "        Returns:\n",
    "            Negative log_probability (batch_size,)\n",
    "        \"\"\"\n",
    "\n",
    "        return -self.log_prob(input, condition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
